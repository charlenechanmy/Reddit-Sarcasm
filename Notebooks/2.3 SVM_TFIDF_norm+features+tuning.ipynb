{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "7N61BmRGjSjF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "NCyMz6nVmisi"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"norm_train_data.uu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "SpqlubYylW-l",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807647, 36)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(\"norm_test_data.uu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202166, 35)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(x):\n",
    "    if x == 'neu':\n",
    "        return 0\n",
    "    elif x == 'pos':\n",
    "        return 1\n",
    "    elif x == 'neg': \n",
    "        return -1\n",
    "\n",
    "data['comment_sentiment'] = data['comment_sentiment'].apply(sentiment)\n",
    "test_data['comment_sentiment'] = test_data['comment_sentiment'].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'label', 'comment', 'author', 'subreddit', 'score', 'ups',\n",
       "       'downs', 'date', 'created_utc', 'parent_comment', 'lemmatized_comment',\n",
       "       'lemmatized_parent_comment', 'clean_comment', 'clean_parent_comment',\n",
       "       'lemmatized_clean_comment', 'lemmatized_clean_parent_comment',\n",
       "       'vectorized_comment', 'vectorized_clean_comment',\n",
       "       'vectorized_parent_comment', 'vectorized_clean_parent_comment',\n",
       "       'cosine_similarity_dirty_comments', 'cosine_similarity_clean_comments',\n",
       "       'word_count', 'punctuation_count', 'has_repeated', 'exclaim_count',\n",
       "       'qns_mark_count', 'ellipses_mark_count', 'interjection_count',\n",
       "       'laughter_words_count', 'capitalized_word_count',\n",
       "       'partial_capital_word_count', 'emoticon_count', 'comment_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.rename(columns = {'dirty_cosine_similarity':'cosine_similarity_dirty_comments'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[['label','clean_comment','cosine_similarity_dirty_comments','word_count','punctuation_count','has_repeated','exclaim_count','qns_mark_count','ellipses_mark_count','interjection_count','laughter_words_count','capitalized_word_count','partial_capital_word_count','emoticon_count','comment_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202166, 15)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "AvKrUGp9mttR",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'label', 'comment', 'author', 'subreddit', 'score', 'ups',\n",
       "       'downs', 'date', 'created_utc', 'parent_comment', 'word_count',\n",
       "       'punctuation_count', 'has_repeated', 'exclaim_count', 'qns_mark_count',\n",
       "       'ellipses_mark_count', 'interjection_count', 'laughter_words_count',\n",
       "       'capitalized_word_count', 'partial_capital_word_count',\n",
       "       'emoticon_count', 'clean_comment', 'lemmatized_comment',\n",
       "       'lemmatized_parent_comment', 'clean_parent_comment',\n",
       "       'lemmatized_clean_comment', 'clean_lemmatized_parent_comment',\n",
       "       'vectorized_comment', 'vectorized_clean_comment', 'vectorized_parent',\n",
       "       'vectorized_clean_parent', 'cosine_similarity_dirty_comments',\n",
       "       'cosine_similarity_clean_comments', 'comment_sentiment',\n",
       "       'parent_comment_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "c5tllih3mjTI"
   },
   "outputs": [],
   "source": [
    "data1 = data.drop(['author','subreddit','date','created_utc','vectorized_comment',\n",
    "                  'vectorized_clean_comment','vectorized_parent','vectorized_clean_parent'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "kgpFQRnMmjVe",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>...</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "      <th>lemmatized_parent_comment</th>\n",
       "      <th>clean_parent_comment</th>\n",
       "      <th>lemmatized_clean_comment</th>\n",
       "      <th>clean_lemmatized_parent_comment</th>\n",
       "      <th>cosine_similarity_dirty_comments</th>\n",
       "      <th>cosine_similarity_clean_comments</th>\n",
       "      <th>comment_sentiment</th>\n",
       "      <th>parent_comment_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417033</td>\n",
       "      <td>0</td>\n",
       "      <td>But it could mean a loss</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We need to take some shots down the field. Bei...</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>But it could mean a loss</td>\n",
       "      <td>But could mean loss</td>\n",
       "      <td>We need take shot field . Being way fuck caref...</td>\n",
       "      <td>We need to take some shots down the field Bein...</td>\n",
       "      <td>But could mean loss</td>\n",
       "      <td>We need take shot field Being way fuck careful...</td>\n",
       "      <td>0.538655</td>\n",
       "      <td>0.552133</td>\n",
       "      <td>-1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59081</td>\n",
       "      <td>0</td>\n",
       "      <td>Donald the reality TV star does care about TV ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>About 71 million watch final presidential deba...</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Donald the reality TV star does care about TV ...</td>\n",
       "      <td>Donald reality TV star care TV rating .</td>\n",
       "      <td>About 71 million watch final presidential deba...</td>\n",
       "      <td>About 71 million watch final presidential deba...</td>\n",
       "      <td>Donald reality TV star care TV rating</td>\n",
       "      <td>About 71 million watch final presidential deba...</td>\n",
       "      <td>0.582335</td>\n",
       "      <td>0.579743</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5664</td>\n",
       "      <td>1</td>\n",
       "      <td>me irl</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>How much you want to bet that the artist was m...</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>me irl</td>\n",
       "      <td>irl</td>\n",
       "      <td>How much want bet artist make regular tiger fu...</td>\n",
       "      <td>How much you want to bet that the artist was m...</td>\n",
       "      <td>irl</td>\n",
       "      <td>How much want bet artist make regular tiger fu...</td>\n",
       "      <td>0.515965</td>\n",
       "      <td>0.510553</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366838</td>\n",
       "      <td>1</td>\n",
       "      <td>Can't have a discussion on one without the oth...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>And of course there are the \"what about circum...</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Cant have a discussion on one without the othe...</td>\n",
       "      <td>Ca n't discussion one without , right ?</td>\n",
       "      <td>And course `` circumcision ? '' asshole ca n't...</td>\n",
       "      <td>And of course there are the what about circumc...</td>\n",
       "      <td>Cant discussion one without right</td>\n",
       "      <td>And course circumcision assholes cant thing</td>\n",
       "      <td>0.533188</td>\n",
       "      <td>0.423121</td>\n",
       "      <td>0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>907940</td>\n",
       "      <td>1</td>\n",
       "      <td>That dude's friend's life is nowhere near as i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maybe he found out that one of his friends lik...</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>That dudes friends life is nowhere near as imp...</td>\n",
       "      <td>That dude 's friend 's life nowhere near impor...</td>\n",
       "      <td>Maybe find one friend like . He n't want embar...</td>\n",
       "      <td>Maybe he found out that one of his friends lik...</td>\n",
       "      <td>That dudes friend life nowhere near important ...</td>\n",
       "      <td>Maybe find one friend like He doesnt want emba...</td>\n",
       "      <td>0.496283</td>\n",
       "      <td>0.471443</td>\n",
       "      <td>-1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  label                                            comment  score  \\\n",
       "0  417033      0                           But it could mean a loss    1.0   \n",
       "1   59081      0  Donald the reality TV star does care about TV ...    1.0   \n",
       "2    5664      1                                             me irl    1.0   \n",
       "3  366838      1  Can't have a discussion on one without the oth...   55.0   \n",
       "4  907940      1  That dude's friend's life is nowhere near as i...    1.0   \n",
       "\n",
       "    ups  downs                                     parent_comment  word_count  \\\n",
       "0   1.0    0.0  We need to take some shots down the field. Bei...    0.002251   \n",
       "1  -1.0   -1.0  About 71 million watch final presidential deba...    0.004052   \n",
       "2  -1.0   -1.0  How much you want to bet that the artist was m...    0.000450   \n",
       "3  55.0    0.0  And of course there are the \"what about circum...    0.004052   \n",
       "4   1.0    0.0  Maybe he found out that one of his friends lik...    0.011256   \n",
       "\n",
       "   punctuation_count  has_repeated  ...  \\\n",
       "0           0.000000           0.0  ...   \n",
       "1           0.000102           0.0  ...   \n",
       "2           0.000000           0.0  ...   \n",
       "3           0.000306           0.0  ...   \n",
       "4           0.000510           0.0  ...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0                           But it could mean a loss   \n",
       "1  Donald the reality TV star does care about TV ...   \n",
       "2                                             me irl   \n",
       "3  Cant have a discussion on one without the othe...   \n",
       "4  That dudes friends life is nowhere near as imp...   \n",
       "\n",
       "                                  lemmatized_comment  \\\n",
       "0                                But could mean loss   \n",
       "1            Donald reality TV star care TV rating .   \n",
       "2                                                irl   \n",
       "3            Ca n't discussion one without , right ?   \n",
       "4  That dude 's friend 's life nowhere near impor...   \n",
       "\n",
       "                           lemmatized_parent_comment  \\\n",
       "0  We need take shot field . Being way fuck caref...   \n",
       "1  About 71 million watch final presidential deba...   \n",
       "2  How much want bet artist make regular tiger fu...   \n",
       "3  And course `` circumcision ? '' asshole ca n't...   \n",
       "4  Maybe find one friend like . He n't want embar...   \n",
       "\n",
       "                                clean_parent_comment  \\\n",
       "0  We need to take some shots down the field Bein...   \n",
       "1  About 71 million watch final presidential deba...   \n",
       "2  How much you want to bet that the artist was m...   \n",
       "3  And of course there are the what about circumc...   \n",
       "4  Maybe he found out that one of his friends lik...   \n",
       "\n",
       "                            lemmatized_clean_comment  \\\n",
       "0                                But could mean loss   \n",
       "1              Donald reality TV star care TV rating   \n",
       "2                                                irl   \n",
       "3                  Cant discussion one without right   \n",
       "4  That dudes friend life nowhere near important ...   \n",
       "\n",
       "                     clean_lemmatized_parent_comment  \\\n",
       "0  We need take shot field Being way fuck careful...   \n",
       "1  About 71 million watch final presidential deba...   \n",
       "2  How much want bet artist make regular tiger fu...   \n",
       "3        And course circumcision assholes cant thing   \n",
       "4  Maybe find one friend like He doesnt want emba...   \n",
       "\n",
       "   cosine_similarity_dirty_comments  cosine_similarity_clean_comments  \\\n",
       "0                          0.538655                          0.552133   \n",
       "1                          0.582335                          0.579743   \n",
       "2                          0.515965                          0.510553   \n",
       "3                          0.533188                          0.423121   \n",
       "4                          0.496283                          0.471443   \n",
       "\n",
       "  comment_sentiment parent_comment_sentiment  \n",
       "0                -1                      pos  \n",
       "1                 1                      neg  \n",
       "2                 0                      neg  \n",
       "3                 0                      neg  \n",
       "4                -1                      pos  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "oNbhygHZn8Ee",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>cosine_similarity_dirty_comments</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>qns_mark_count</th>\n",
       "      <th>ellipses_mark_count</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>laughter_words_count</th>\n",
       "      <th>capitalized_word_count</th>\n",
       "      <th>partial_capital_word_count</th>\n",
       "      <th>emoticon_count</th>\n",
       "      <th>comment_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>But it could mean a loss</td>\n",
       "      <td>0.538655</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Donald the reality TV star does care about TV ...</td>\n",
       "      <td>0.582335</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>me irl</td>\n",
       "      <td>0.515965</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cant have a discussion on one without the othe...</td>\n",
       "      <td>0.533188</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>That dudes friends life is nowhere near as imp...</td>\n",
       "      <td>0.496283</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>So antiMormon lies then</td>\n",
       "      <td>0.410225</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Fiddlesticks rip off</td>\n",
       "      <td>0.472955</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>I dont have neither an X1 nor an X360 so they ...</td>\n",
       "      <td>0.558923</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah this would be so convenient to carry around</td>\n",
       "      <td>0.650354</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>You forgot</td>\n",
       "      <td>0.505771</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>minigames are staying no worries</td>\n",
       "      <td>0.407850</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Change the word allowed to required and youve ...</td>\n",
       "      <td>0.530434</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Winning hearts and minds</td>\n",
       "      <td>0.602078</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>Wrong sub jerker</td>\n",
       "      <td>0.613339</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>But but but but Denovo bad cant paly games whe...</td>\n",
       "      <td>0.516148</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>Ooh yeah solve for that lambda in terms of mic...</td>\n",
       "      <td>0.373375</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>HL3 games for windows  a single tear was shed ...</td>\n",
       "      <td>0.625256</td>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Stop ruining my unrealistic Hollywood expectat...</td>\n",
       "      <td>0.533007</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>reminds me of a dragon mace but better</td>\n",
       "      <td>0.524504</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Another Sony Fail</td>\n",
       "      <td>0.521173</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                      clean_comment  \\\n",
       "0       0                           But it could mean a loss   \n",
       "1       0  Donald the reality TV star does care about TV ...   \n",
       "2       1                                             me irl   \n",
       "3       1  Cant have a discussion on one without the othe...   \n",
       "4       1  That dudes friends life is nowhere near as imp...   \n",
       "5       1                            So antiMormon lies then   \n",
       "6       1                               Fiddlesticks rip off   \n",
       "7       1  I dont have neither an X1 nor an X360 so they ...   \n",
       "8       1   Yeah this would be so convenient to carry around   \n",
       "9       1                                         You forgot   \n",
       "10      0                   minigames are staying no worries   \n",
       "11      1  Change the word allowed to required and youve ...   \n",
       "12      1                           Winning hearts and minds   \n",
       "13      0                                   Wrong sub jerker   \n",
       "14      1  But but but but Denovo bad cant paly games whe...   \n",
       "15      0  Ooh yeah solve for that lambda in terms of mic...   \n",
       "16      0  HL3 games for windows  a single tear was shed ...   \n",
       "17      1  Stop ruining my unrealistic Hollywood expectat...   \n",
       "18      0             reminds me of a dragon mace but better   \n",
       "19      0                                  Another Sony Fail   \n",
       "\n",
       "    cosine_similarity_dirty_comments  word_count  punctuation_count  \\\n",
       "0                           0.538655    0.002251           0.000000   \n",
       "1                           0.582335    0.004052           0.000102   \n",
       "2                           0.515965    0.000450           0.000000   \n",
       "3                           0.533188    0.004052           0.000306   \n",
       "4                           0.496283    0.011256           0.000510   \n",
       "5                           0.410225    0.001351           0.000612   \n",
       "6                           0.472955    0.000900           0.000102   \n",
       "7                           0.558923    0.006303           0.000306   \n",
       "8                           0.650354    0.003602           0.000000   \n",
       "9                           0.505771    0.000450           0.000000   \n",
       "10                          0.407850    0.001801           0.000102   \n",
       "11                          0.530434    0.004052           0.000612   \n",
       "12                          0.602078    0.001351           0.000000   \n",
       "13                          0.613339    0.000900           0.000102   \n",
       "14                          0.516148    0.006754           0.000102   \n",
       "15                          0.373375    0.005403           0.000102   \n",
       "16                          0.625256    0.004953           0.000306   \n",
       "17                          0.533007    0.002251           0.000102   \n",
       "18                          0.524504    0.003152           0.000102   \n",
       "19                          0.521173    0.000900           0.000000   \n",
       "\n",
       "    has_repeated  exclaim_count  qns_mark_count  ellipses_mark_count  \\\n",
       "0            0.0       0.000000        0.000000               0.0000   \n",
       "1            0.0       0.000000        0.000000               0.0000   \n",
       "2            0.0       0.000000        0.000000               0.0000   \n",
       "3            0.0       0.000000        0.111111               0.0000   \n",
       "4            0.0       0.000000        0.000000               0.0000   \n",
       "5            0.0       0.000000        0.111111               0.0625   \n",
       "6            0.0       0.022727        0.000000               0.0000   \n",
       "7            0.0       0.000000        0.000000               0.0000   \n",
       "8            0.0       0.000000        0.000000               0.0000   \n",
       "9            0.0       0.000000        0.000000               0.0000   \n",
       "10           0.0       0.000000        0.000000               0.0000   \n",
       "11           0.0       0.000000        0.000000               0.0000   \n",
       "12           0.0       0.000000        0.000000               0.0000   \n",
       "13           0.0       0.000000        0.000000               0.0000   \n",
       "14           0.0       0.000000        0.000000               0.0000   \n",
       "15           0.0       0.000000        0.000000               0.0000   \n",
       "16           0.0       0.000000        0.000000               0.0000   \n",
       "17           0.0       0.000000        0.000000               0.0000   \n",
       "18           0.0       0.000000        0.000000               0.0000   \n",
       "19           0.0       0.000000        0.000000               0.0000   \n",
       "\n",
       "    interjection_count  laughter_words_count  capitalized_word_count  \\\n",
       "0             0.000000                   0.0                0.000000   \n",
       "1             0.000000                   0.0                0.001203   \n",
       "2             0.000000                   0.0                0.000000   \n",
       "3             0.000000                   0.0                0.000000   \n",
       "4             0.000000                   0.0                0.000000   \n",
       "5             0.000000                   0.0                0.000000   \n",
       "6             0.000000                   0.0                0.000000   \n",
       "7             0.000000                   0.0                0.001203   \n",
       "8             0.000000                   0.0                0.000000   \n",
       "9             0.000000                   0.0                0.000000   \n",
       "10            0.000000                   0.0                0.000000   \n",
       "11            0.000000                   0.0                0.000000   \n",
       "12            0.000000                   0.0                0.000000   \n",
       "13            0.000000                   0.0                0.000000   \n",
       "14            0.000000                   0.0                0.000000   \n",
       "15            0.166667                   0.0                0.000000   \n",
       "16            0.000000                   0.0                0.000601   \n",
       "17            0.000000                   0.0                0.000000   \n",
       "18            0.000000                   0.0                0.000000   \n",
       "19            0.000000                   0.0                0.000000   \n",
       "\n",
       "    partial_capital_word_count  emoticon_count  comment_sentiment  \n",
       "0                          0.0             0.0                 -1  \n",
       "1                          0.0             0.0                  1  \n",
       "2                          0.0             0.0                  0  \n",
       "3                          0.0             0.0                  0  \n",
       "4                          0.0             0.0                 -1  \n",
       "5                          0.0             0.0                 -1  \n",
       "6                          0.0             0.0                  0  \n",
       "7                          0.0             0.0                  1  \n",
       "8                          0.0             0.0                  1  \n",
       "9                          0.0             0.0                  0  \n",
       "10                         0.0             0.0                  1  \n",
       "11                         0.0             0.0                  0  \n",
       "12                         0.0             0.0                  1  \n",
       "13                         0.0             0.0                 -1  \n",
       "14                         0.0             0.0                 -1  \n",
       "15                         0.0             0.0                  1  \n",
       "16                         0.0             0.0                  0  \n",
       "17                         0.0             0.0                 -1  \n",
       "18                         0.0             0.0                  1  \n",
       "19                         0.0             0.0                 -1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try SVM with just the cleaned comments + feature selections\n",
    "data_try = data1[['label','clean_comment','cosine_similarity_dirty_comments','word_count','punctuation_count','has_repeated','exclaim_count','qns_mark_count','ellipses_mark_count','interjection_count','laughter_words_count','capitalized_word_count','partial_capital_word_count','emoticon_count','comment_sentiment']]\n",
    "data_try.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "DwAbsQK1ob-c"
   },
   "outputs": [],
   "source": [
    " #split\n",
    "#  X_train, X_test, y_train, y_test = train_test_split(data_try.drop('label', axis = 1), data_try.label.astype(int), \n",
    "#                                                     stratify=data_try.label.astype(int), \n",
    "#                                                    random_state=42, \n",
    "#                                                    test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807647, 15)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_try.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning via Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC()\n",
    "tfidf_1 = TfidfVectorizer()\n",
    "clmn = ColumnTransformer([(\"tfidf_1\", tfidf_1, \"clean_comment\")\n",
    "                         ],\n",
    "                         remainder=\"passthrough\")\n",
    "\n",
    "pipe_cv = Pipeline([\n",
    "                  ('tfidf', clmn),\n",
    "                  ('classify', lsvc)\n",
    "                ],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {\n",
    "    'classify__C': [0.1, 0.3, 0.5, 0.8, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.7s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   3.5s\n",
      "[CV] END ....................................classify__C=0.1; total time=  11.9s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.7s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   3.6s\n",
      "[CV] END ....................................classify__C=0.1; total time=  11.9s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.6s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   3.6s\n",
      "[CV] END ....................................classify__C=0.1; total time=  11.9s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.6s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   6.8s\n",
      "[CV] END ....................................classify__C=0.3; total time=  15.1s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.6s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   6.9s\n",
      "[CV] END ....................................classify__C=0.3; total time=  15.1s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.6s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   6.8s\n",
      "[CV] END ....................................classify__C=0.3; total time=  15.1s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.7s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  10.2s\n",
      "[CV] END ....................................classify__C=0.5; total time=  18.5s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.6s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  11.6s\n",
      "[CV] END ....................................classify__C=0.5; total time=  20.1s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   6.0s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  10.4s\n",
      "[CV] END ....................................classify__C=0.5; total time=  19.1s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   6.8s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  15.6s\n",
      "[CV] END ....................................classify__C=0.8; total time=  25.2s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.7s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  16.2s\n",
      "[CV] END ....................................classify__C=0.8; total time=  24.6s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   6.0s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  15.5s\n",
      "[CV] END ....................................classify__C=0.8; total time=  24.3s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.7s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  19.5s\n",
      "[CV] END ......................................classify__C=1; total time=  28.0s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   6.0s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  19.6s\n",
      "[CV] END ......................................classify__C=1; total time=  28.3s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   5.7s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=  18.8s\n",
      "[CV] END ......................................classify__C=1; total time=  27.2s\n",
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   8.6s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   5.9s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "svc_random = RandomizedSearchCV(pipe_cv, param_distributions = random_grid, n_iter = 5, cv = 3, verbose=2, random_state=42, n_jobs = 1)\n",
    "search = svc_random.fit(data_try.drop('label', axis = 1),data_try.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classify__C': 0.1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_kClYOYDE_x"
   },
   "source": [
    "# Fitting svm (LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "_NZD2MEppfoK"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "#TODO: tune\n",
    "\n",
    "# model = SVC(C=1.0, probability=True)\n",
    "# model = SVC(C=1.0)\n",
    "lsvc = LinearSVC(C=0.1)\n",
    "tfidf_1 = TfidfVectorizer()\n",
    "clmn = ColumnTransformer([(\"tfidf_1\", tfidf_1, \"clean_comment\")\n",
    "                         ],\n",
    "                         remainder=\"passthrough\")\n",
    "\n",
    "pipe = Pipeline([\n",
    "                  ('tfidf', clmn),\n",
    "                  ('classify', lsvc)\n",
    "                ],verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(807647, 15)\n",
      "(202166, 15)\n"
     ]
    }
   ],
   "source": [
    "print(data_try.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Sx9tsK6qmJZ",
    "outputId": "84d84bbf-4b19-4277-e350-790008788f28",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   9.1s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   5.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('tfidf_1', TfidfVectorizer(),\n",
       "                                                  'clean_comment')])),\n",
       "                ('classify', LinearSVC(C=0.1))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(data_try.drop('label', axis = 1),data_try.label) # 4 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "zYJT_UhzqnvN"
   },
   "outputs": [],
   "source": [
    "def metrics(test_data, prediction):\n",
    "  cm = pd.DataFrame(confusion_matrix(test_data, prediction))\n",
    "  cm.columns = ['Predicted Y=0','Predicted Y=1']\n",
    "  cm.index = ['True Y=0','True Y=1']\n",
    "  display(cm)\n",
    "\n",
    "  accuracy = (cm.iloc[0,0]+cm.iloc[1,1])/(cm.iloc[0,0]+cm.iloc[1,1]+cm.iloc[0,1]+cm.iloc[1,0])\n",
    "  print('Accuracy: '+str(accuracy))\n",
    "\n",
    "  # Possible ways to further improve accuracy is to try adjusting the level of significance.\n",
    "\n",
    "  # Calculate Sensitivity (true positive rate)\n",
    "  sensitivity = cm.iloc[1, 1]/(cm.iloc[1, 1] + cm.iloc[1, 0])\n",
    "  print('Sensitivity: '+ str(sensitivity))\n",
    "\n",
    "  # Calculate Specificity (true negative rate)\n",
    "  specificity = cm.iloc[0, 0]/(cm.iloc[0, 0] + cm.iloc[0, 1])\n",
    "  print('Specificity: '+str(specificity))\n",
    "\n",
    "  #Precision\n",
    "  precision = cm.iloc[1,1]/(cm.iloc[1,1]+cm.iloc[0,1])\n",
    "  print('Precision: '+str(precision))\n",
    "\n",
    "  # AUC\n",
    "  print('ROC-AUC:',roc_auc_score(test_data, prediction))\n",
    "\n",
    "  #F1 Score\n",
    "  print(\"F1 score:\", round(f1_score(test_data, prediction), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "bvkSdmtLqnxj",
    "outputId": "90d43047-a589-4353-b185-ff1fe0ebdbc6",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Y=0</th>\n",
       "      <th>Predicted Y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Y=0</th>\n",
       "      <td>73514</td>\n",
       "      <td>27441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Y=1</th>\n",
       "      <td>33836</td>\n",
       "      <td>67375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted Y=0  Predicted Y=1\n",
       "True Y=0          73514          27441\n",
       "True Y=1          33836          67375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6968975990027997\n",
      "Sensitivity: 0.665688512118248\n",
      "Specificity: 0.7281858253677381\n",
      "Precision: 0.7105868207897401\n",
      "ROC-AUC: 0.696937168742993\n",
      "F1 score: 0.6874\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(test_data.drop('label',axis=1))\n",
    "metrics(test_data.label.astype(int),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_pred_norm+features+tuning.npy', 'wb') as f:\n",
    "    np.save(f, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_pred_norm+features+tuning.npy', 'rb') as f:\n",
    "    y_pred_norm_features_tuning = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_norm_features_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#TODO: tune\n",
    "\n",
    "# model = SVC(C=1.0, probability=True)\n",
    "# mnb = MultinomialNB()\n",
    "mnb = BernoulliNB()\n",
    "tfidf_1 = CountVectorizer()\n",
    "mm = MinMaxScaler()\n",
    "clmn = ColumnTransformer([(\"tfidf_1\", tfidf_1, \"clean_comment\")\n",
    "                         ],\n",
    "                         remainder=\"passthrough\")\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "                  ('tfidf', clmn),\n",
    "                  ('classify', mnb)\n",
    "                ],verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807647, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_try.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=   7.1s\n",
      "[Pipeline] .......... (step 2 of 2) Processing classify, total=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('tfidf_1', CountVectorizer(),\n",
       "                                                  'clean_comment')])),\n",
       "                ('classify', BernoulliNB())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_nb.fit(data_try.drop('label', axis = 1),data_try.label) # fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Y=0</th>\n",
       "      <th>Predicted Y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Y=0</th>\n",
       "      <td>71170</td>\n",
       "      <td>29785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Y=1</th>\n",
       "      <td>33334</td>\n",
       "      <td>67877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted Y=0  Predicted Y=1\n",
       "True Y=0          71170          29785\n",
       "True Y=1          33334          67877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6877862746455883\n",
      "Sensitivity: 0.6706484473031588\n",
      "Specificity: 0.704967559803873\n",
      "Precision: 0.6950195572484692\n",
      "ROC-AUC: 0.6878080035535159\n",
      "F1 score: 0.6826\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe_nb.predict(test_data.drop('label',axis=1))\n",
    "metrics(test_data.label.astype(int),y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SVM-TFIDF-17nov.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
